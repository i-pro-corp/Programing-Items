<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="author" content="kinoshita hidetosi (木下英俊)">
  <meta name="description" content="Introducing programming for i-PRO cameras.">
  <meta name="keywords" content="i-PRO">
  
  <!-- キャッシュ無効化 -->
  <meta http-equiv="Cache-Control" content="no-cache">
	
  <!-- タイトル -->
  <title>物体検知 － MobileNet-SSD (推論編) | i-PRO - Programming Items</title>

  <!-- ファビコン -->
  <link rel="shortcut icon" href="../../favicon.ico">
	
  <!-- CSS -->
  <link href="https://unpkg.com/ress/dist/ress.min.css" rel="stylesheet">
	<link rel="stylesheet" href="../../design.css" type="text/css">
  
	<!-- Start for 'google-code-prettify' -->
	<link href="../../prettify/styles/desert.css" rel="stylesheet" type="text/css">
	<script src="../../prettify/prettify.js" type="text/javascript"></script>
	<!-- End for 'google-code-prettify' -->	
	
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5DFRG3H0KB"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5DFRG3H0KB');
  </script>  
  <!-- Global site tag (gtag.js) - Google Analytics -->

  <style type="text/css">
    .auto-style1 {
      background-color: #505000;
    }
    .auto-style2 {
    text-decoration: underline;
    }
    .auto-style3 {
      color: #FF0000;
    }
    .auto-style6 {
      color: #FF0000;
      background-color: #505000;
    }
  </style>
</head>

<body onload="prettyPrint();">
	
<h1>物体検知 － MobileNet-SSD (推論編)</h1>

<p>&nbsp;</p>

<div class="status_information">
  <div>
  </div>
  <div>
    <p>本ページは i-PRO株式会社 の有志メンバーにより記載されたものです。<br>本ページの情報は <a href="#ライセンス">ライセンス</a> に記載の条件で提供されます。</p>
  </div>
</div>

<p>&nbsp;</p>

<div class="mokuji">
  <nav>
    <h2>目次</h2>
    <p><a href="#1._準備">1. 準備</a></p>
    <p>&nbsp;&nbsp; <a href="#1-1._Pytorch_をインストールする">1-1. PyTorch をインストールする</a></p>
    <p>&nbsp;&nbsp; <a href="#1-2._必要なライブラリをインストール">1-2. 必要なライブラリをインストールする</a></p>
    <p><a href="#2._MobileNetV1-SSD">2. MobileNetV1-SSD</a></p>
    <p>&nbsp;&nbsp; <a href="#2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">2-1. MobileNetV1-SSD を動かす（PC 内蔵カメラ）</a></p>
    <p>&nbsp;&nbsp; <a href="#2-2._MobileNetV1-SSD_を動かす（i-PRO_カメラ）">2-2. MobileNetV1-SSD を動かす（i-PRO カメラ）</a></p>
    <p><a href="#3._MobileNetV2-SSD-Lite">3. MobileNetV2-SSD-Lite</a></p>
    <p>&nbsp;&nbsp; <a href="#3-1._MobileNetV2-SSD-Lite_を動かす（PC_内蔵カメラ）">3-1. MobileNetV2-SSD-Lite を動かす（PC 内蔵カメラ）</a></p>
    <p>&nbsp;&nbsp; <a href="#3-2._MobileNetV2-SSD-Lite_を動かす（i-PRO_カメラ）">3-2. MobileNetV2-SSD-Lite を動かす（i-PRO カメラ）</a></p>
    <p>&nbsp;&nbsp; <a href="#参考：_GPU動作させる場合のソースコード修正について">参考： GPU動作させる場合のソースコード修正について</a></p>
  	<p><a href="#4._MobileNetV3-SSD-Lite">4. MobileNetV3-SSD-Lite</a></p>
    <p>&nbsp;&nbsp; <a href="#4-1._ssdlite320_mobilenet_v3_large">4-1. ssdlite320 mibilenet v3 large</a>&nbsp;</p>
  	<p>&nbsp;&nbsp;&nbsp;&nbsp; <a href="#4-1-1._MobileNetV3-SSD-Lite_を動かす（JPEGファイル）">4-1-1. MobileNetV3-SSD-Lite を動かす（JPEGファイル）</a></p>
  	<p>&nbsp;&nbsp;&nbsp;&nbsp; <a href="#4-1-2._MobileNetV3-SSD-Lite_を動かす（PC_内蔵カメラ）">4-1-2. MobileNetV3-SSD-Lite を動かす（PC 内蔵カメラ）</a></p>
  	<p>&nbsp;&nbsp;&nbsp;&nbsp; <a href="#4-1-3._MobileNetV3-SSD-Lite_を動かす（i-PRO_内蔵カメラ）">4-1-3. MobileNetV3-SSD-Lite を動かす（i-PRO カメラ）</a></p>
    <p>&nbsp;&nbsp; <a href="#4-2._MobileNetV3-large-SSD-Lite">4-2. MobileNetV3-large-SSD-Lite</a></p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; <a href="#4-2-1._MobileNetV3-large-SSD-Lite_学習データを作る">4-2-1. MobileNetV3-large-SSD-Lite 学習データを作る</a></p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; <a href="#4-2-2._MobileNetV3-large-SSD-Lite_を動かす（PC_内蔵カメラ）">4-2-2. MobileNetV3-large-SSD-Lite を動かす（PC 内蔵カメラ）</a></p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; <a href="#4-2-3._MobileNetV3-large-SSD-Lite_を動かす（i-PRO_カメラ）">4-2-3. MobileNetV3-large-SSD-Lite を動かす（i-PRO カメラ）</a></p>
    <p>&nbsp;</p>
	  <p>=== 5章は別ページ「<a href="mobilenet-ssd_train.html">物体検知 － MobileNet-SSD (学習編)</a>」で記載 ===</p>
	  <p>5. 学習</p>
    <p>&nbsp;&nbsp; 5-1. まずはやってみる (Open Images)</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-1-1. 準備</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-1-2. 学習</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-1-3. 学習結果と静止画を使って推論</p>
    <p>&nbsp;&nbsp; 5-2. 独自の画像を学習してみる (Pascal/VOC)</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-2-1. 準備</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-2-2. アノテーション</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-2-3. "trainval.txt"、"test.txt" を用意</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-2-4. 学習</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-2-5. 学習結果と静止画を使って推論&nbsp;</p>
    <p>&nbsp;&nbsp;&nbsp;&nbsp; 5-2-6. 学習結果とカメラを使って物体検知してみる (i-PRO カメラ)</p>
    <p>6. 後書き</p>
    <br>
  	<p><a href="#ソースコード所在">ソースコード所在</a></p>
    <p><a href="#ライセンス">ライセンス</a></p>
    <p><a href="#参考">参考</a></p>
  </nav>
</div>

<p>&nbsp;</p>
<p><strong>MobileNet-SSD</strong> は、高速に物体物体検知を行うAIモデルの一つです。高い認識性能と共に GPU 
を搭載しない組み込み機器でも動作する軽量なモデルであることに特徴があります。</p>
<p>本ページでは、<strong>MobileNet-SSD</strong> を使ってカメラ映像を画像処理する方法について記載します。</p>
<p>MobileNetSSD は V1, V2, V3 まで発表されていますので、これらを１つずつ動作させてみたいと思います。i-PRO 
カメラと接続して使用する手順についても具体的に紹介していきます。</p>
<p>&nbsp;</p>
<p>こちら、私のノートPCで CPU 動作させた例です。GPU無しの動作環境ですがこれぐらいでリアルタイム動作できています。</p>

<video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv2-ssd-pccam.mp4" width="800">
  <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
</video>
    
<p>&nbsp;</p>

<video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv3-ssd-ipromini_1.mp4" width="800">
  <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
</video>
    
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
  <p class="auto-style2"> <strong>"i-PRO mini" 紹介： </strong> </p>
  <ul>
    <li><a href="https://cwc.i-pro.com/pages/i-pro-mini-lp" target="_blank">
      i-PRO mini</a></li>
    <li><a href="https://cwc.i-pro.com/collections/camera/products/wv-s7130ux" target="_blank">
      i-PRO mini 有線LANモデル WV-S7130UX</a></li>
    <li>  <a href="https://cwc.i-pro.com/collections/camera/products/wv-s7130wux" target="_blank">
      i-PRO mini 無線LANモデル WV-S7130WUX</a></li>
    <li><a href="https://japancs.i-pro.com/space/DLJP/724085590/WV-S7130UX　i-PRO+mini+有線LANモデル" target="_blank">
      WV-S7130UX　i-PRO mini 有線LANモデル - ダウンロード - i-PRO サポートポータル</a></li>
    <li><a href="https://japancs.i-pro.com/space/DLJP/724086255/WV-S7130WUX　i-PRO+mini+無線LANモデル" target="_blank">
      WV-S7130WUX　i-PRO mini 無線LANモデル - ダウンロード - i-PRO サポートポータル</a></li>
  </ul>
  
  <p> 
  <a href="images/i-PRO_mini.jpg" target="_blank">
  <img alt="i-PRO mini 画像" src="images/i-PRO_mini.jpg" class="border_with_drop-shadow" width="348"></a></p>
  
  <p>&nbsp;</p>
  <p class="auto-style2"><strong>"モジュールカメラ" 紹介：</strong></p>
  <ul>
    <li><a href="https://moduca.i-pro.com" target="_blank">
      モジュールカメラ｜ポータルサイト (i-pro.com)</a></li>
    <li><a href="https://moduca.i-pro.com/space/MCT/768743132/各種マニュアル" target="_blank">
      各種マニュアル - Module Camera Technical Information - モジュールカメラ｜ポータルサイト (i-pro.com)</a></li>
  </ul>
  
  <p> 
  <a href="images/ai_starter_kit_1.png" target="_blank">
  <img alt="AIスターターキット画像（その１）" class="border_with_drop-shadow" src="images/ai_starter_kit_1.png" width="404"></a>
  <a href="images/ai_starter_kit_2.png" target="_blank">
  <img alt="AIスターターキット画像（その２）" class="border_with_drop-shadow" src="images/ai_starter_kit_2.png" width="444"></a></p>

  <p>&nbsp;</p>
  <p>カメラ初期設定についてはカメラ毎の取扱説明書をご確認ください。</p>
  <p>カメラのIPアドレスを確認・設定できる下記ツールを事前に入手しておくと便利です。</p>

  <ul>
    <li>
    <a href="https://connect.panasonic.com/jp-ja/products-services_security_support_specifications-manuals-firms-tool_2014040315191048" target="_blank">
    IP簡単設定ソフトウェア</a>&nbsp;（日本国内）</li>
    <li>
    <a href="https://bizpartner.panasonic.net/public/file/ip-setting-software" target="_blank">
    IP Setting Software</a>&nbsp;&nbsp;&nbsp;&nbsp; （グローバル）</li>
  </ul>
</section>

<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
	<h2> <a name="1._準備">1. 準備</a></h2>
	<h4>[概要]</h4>
    <p>Python を事前にインストール済みであることを前提に記載します。</p>
    <p>私の評価環境は以下の通りです。</p>
  <p>&nbsp;</p>
	
	<h4>[評価環境]</h4>
	<table>
	<tbody>
	  <tr>
	    <td class="td_separate" colspan="3"></td>
	  </tr>
		
	  <tr>
	    <td>言語 :</td>
	    <td>Python,</td>
	    <td>3.10.4 </td>
	  </tr>
		
	  <tr>
	    <td class="td_separate" colspan="3"></td>
	  </tr>
		
	  <tr>
	    <td>OS :</td>
	    <td>Windows 11 home,</td>
	    <td>21H2</td>
	  </tr>
		
	  <tr>
	    <td>&nbsp;</td>
	    <td>Windows 10 pro,</td>
	    <td>21H2</td>
	  </tr>
		
	  <tr>
	    <td class="td_separate" colspan="3"></td>
	  </tr>
	  
	</tbody>
	</table>
	
	<p>&nbsp;</p>
	<p> &nbsp;</p>
  <h3> <a name="1-1._Pytorch_をインストールする">1-1. PyTorch をインストールする</a></h3>
  <p> こちら「<a href="../install_pytorch.html#1-1._CPU">PyTorch をインストールする － 1-1. CPU</a>」の記事を参考にインストールを行います。</p>
  <p> Windows 環境で多くの人が試せるようにしたいので、本ページでは「Compute 
  Platform」を「CPU」としてインストールしている前提で説明を記載します。NVIDIA の GPU など特定のハードウェアを必要としません。</p>
  <p> &nbsp;</p>
  <p> &nbsp;</p>
  <h3> <a name="1-2._必要なライブラリをインストール">1-2. 必要なライブラリをインストール</a></h3>
  <p> opencv を使用するので、下記コマンドによりインストールします。</p>
  <p> <span class="cpp-source">pip3 install opencv-python </span></p>
	
</section>
	
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<h2><a name="2._MobileNetV1-SSD">2. MobileNetV1-SSD</a></h2>
<p>2017年に MobileNet v1 が発表されました。（<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank">MobileNet 
V1 の原著論文</a>）</p>
<p>
分類・物体検出・セマンティックセグメンテーションを含む画像認識を、モバイル端末などの限られたリソース下で高精度で判別するモデルを作成することを目的として作成しています。</p>
<p>MobileNetV1 は下記２つの技術により高速化を行っています。</p>
<ol>
  <li>Depthwise Separable Convolution</li>
  <li>Pointwise convolution</li>
</ol>
<p>&nbsp;</p>
<ul>
  <li>通常の畳み込み層演算処理を空間方向とチャンネル方向の2段階に分けて行う</li>
  <li>各チャンネル毎に独立して空間方向 (Depthwise, 3x3) のみに畳み込み演算を行う (Depthwise Separable Convolution)</li>
  <li>1x1フィルターの畳み込みによりチャンネル方向 (Pointwise, 1x1) のみに畳み込み演算を行う (Pointwise convolution)</li>
  <li>以上の結果、総演算量を 1/8～1/9 に削減</li>
</ul>
<p>&nbsp;</p>
<p><img alt="MobileNetV1 アーキテクチャ構造" src="mobilenet-ssd/img28.jpg" width="800"></p>
<p>&nbsp;</p>
<p>MobileNet(V1) のアーキテクチャ構造です。</p>
<p><img alt="MobileNetV1 Body Architecture" class="border" src="mobilenet-ssd/img3A.jpg"></p>
<p>&nbsp;</p>
<p>MobileNet(V1) の物体検出性能を、COCOデータセット上で評価・比較した結果です。論文からの引用です。</p>
<p><img alt="COCO object detection results comparison" class="border" src="mobilenet-ssd/img24.jpg" width="600"></p>
<p>引用元： <a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank">MobileNet 
V1 の原著論文</a></p>
<p>&nbsp;</p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table] 用語説明</caption>
      <thead class="standard_table">
      </thead>
      <tbody>
        <tr>
          <td>mAP</td>
          <td>認識性能の平均値（mean Average Precision）</td>
        </tr>
        <tr>
          <td>Mult-Adds</td>
          <td>積和演算の回数</td>
        </tr>
        <tr>
          <td>Parameters</td>
          <td>重みの数</td>
        </tr>
      </tbody>
    </table>
    
<p>&nbsp;</p>
<p>上記結果から、MobileNet(V1) は、認識性能（mAP）を大きく低下することなく計算量（Mult-Adds）を劇的に削減できていることを読み取れます。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
	<h3> <a name="2-1._MobileNetV1-SSD_を動かす（PC 内蔵カメラ）">2-1. MobileNetV1-SSD を動かす（PC 内蔵カメラ）</a></h3>
	<h4>[概要]</h4>
    <p>MobileNetV1-SSD を PyTorch の環境で動かしてみます。</p>
    <p><a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> というこの内容そのままのものが GitHub で公開されていました。これを取得して動作させてみます。</p>
	<p>ちなみに pytorch-ssd のライセンスは "MIT License" です。</p>
    <p><a href="mobilenet-ssd/imgC.jpg" target="_blank">
    <img alt="pytorch-ssd ホームページ画面" class="border_with_drop-shadow" src="mobilenet-ssd/imgC.jpg" width="800"></a></p>
    <p>&nbsp;</p>
	<p> &nbsp;</p>
	
  <h4>[評価環境]</h4>
  <table>
    <tbody>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>言語 :</td>
        <td>Python,</td>
        <td>3.10.4 </td>
      </tr>

      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>&nbsp;</td>
        <td>PyTorch,</td>
        <td>1.11.0+cpu</td>
      </tr>

      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>OS :</td>
        <td>Windows 11 home,</td>
        <td>21H2</td>
      </tr>

      <tr>
        <td>&nbsp;</td>
        <td>Windows 10 pro,</td>
        <td>21H2</td>
      </tr>
      
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
    </tbody>
  </table>

  <p>&nbsp;</p>
  <p>&nbsp;</p>
  <h4>[手順]</h4>
  <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
  pytorch-ssd</a> から git clone してソースコード一式を入手します。</p>
	<p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
  <p>（git を既にインストール済みとして記載します。）</p>
  <pre style="color: #FFFFFF; background-color: #000000; overflow-x: auto;">$ git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
  <p>&nbsp;</p>
	<p>2. "git clone" したフォルダへ移動します。</p>
	<pre style="color: #FFFFFF; background-color: #000000; overflow-x: auto;">$ cd pytorch-ssd</pre>
	<p>&nbsp;</p>
  <p>3. README.md 中の "Run the live MobilenetV1 SSD demo" に記載の内容に従って下記を実行します。</p>
  <p>下記 Google Drive を開いて、"mobilenet-v1-ssd-mp-0_675.pth" および 
  "voc-model-labels.txt" の２つのファイルを取得して models フォルダに保存します。</p>
  <p>
    <a href="https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu" target="_blank">
    https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu</a></p>
  <p>&nbsp;</p>

  <div class="status_information">
    <div>
    </div>
    <div>
      <p><strong>NOTE</strong></p>
      <p>2023/3/11 の 
      <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
      pytorch-ssd</a> 更新によりモデルファイルの取得方法が変更になりました。</p>
    </div>
  </div>

  <p>&nbsp;</p>

  <p>4. 下記コマンドを実行することで推論を実行します。</p>
  <pre style="color: #FFFFFF; background-color: #000000; overflow-x: auto;">$ python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt </pre>
  <p>&nbsp;</p>
	<p>学習済みモデルは VOC dataset を使って学習しているので、このプログラムは下記 20クラス を検知します。</p>
	<pre style="width: 200px">1: aeroplane
2: bicycle
3: bird
4: boat
5: bottle
6: bus
7: car
8: cat
9: chair
10: cow
11: diningtable
12: dog
13: horse
14: motorbike
15: person
16: pottedplant
17: sheep
18: sofa
19: train
20: tvmonitor</pre>

	<p>&nbsp;</p>

  <p>5. 上記手順でプログラム（"python run_ssd_live_demo.py ･･･" のところ）を実行したところ、私の環境では下図のようなエラーを表示して正常に動作しませんでした。</p>
  <p><a href="mobilenet-ssd/img9.gif" target="_blank">
  <img alt="pytorch-ssd － run_ssd_live_demo.py エラー発生時の画面" src="mobilenet-ssd/img9.gif" width="800"></a></p>
  <p>&nbsp;</p>
  <p>どうやら、box[0], box[1], box[2], box[3] 
  が浮動小数点なのですがここの引数は整数(int)である必要がある、ということがエラーの理由のようです。<br>エラーとなった 76行目と、同様に 
  79行目の２カ所を int 型へ変換するように修正します。下図だと4行目、7行目の色付きの場所へ int() を追加しました。<br>
  これでエラーを解決してプログラムを実行できるようになりました。</p>
  <pre class="prettyprint linenums:73 lang-py">
        for i in range(boxes.size(0)):
        box = boxes[i, :]
        label = f"{class_names[labels[i]]}: {probs[i]:.2f}"
        cv2.rectangle(orig_image, (<span class="auto-style1">int(</span>box[0]<span class="auto-style1">)</span>, <span class="auto-style1">int(</span>box[1]<span class="auto-style1">)</span>), (<span class="auto-style1">int(</span>box[2]<span class="auto-style1">)</span>, <span class="auto-style1">int(</span>box[3]<span class="auto-style1">)</span>), (255, 255, 0), 4)

        cv2.putText(orig_image, label,
                    (<span class="auto-style1">int(</span>box[0]<span class="auto-style1">)</span>+20, <span class="auto-style1">int(</span>box[1]<span class="auto-style1">)</span>+40),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    1,  # font scale
                    (255, 0, 255),
                    2)  # line type</pre>
  <p>&nbsp;</p>
	<p>別サイトの記事記載[9] によると、OpenCVバージョンアップに伴う影響だそうです。元々はこのようなエラーは出なかったのでしょう。</p>
	<p>修正を <a href="https://github.com/qfgaohao/pytorch-ssd/pull/178" target="_blank">Pull Request #178</a> しておきましたが、ポスト後に確認したら私を含めて３件の同件修正が Pull Request 
    されていました。こちらのリポジトリはメンテを終了しているかもしれません。</p>
	<p>&nbsp;</p>
    
  <div class="status_ok" style="width: 700px">
    <div></div>
    <div>
      <p>(2022-08-03 追記)</p>
      <p>私から出した Pull Request (#178) が先ほどマージされました。</p>
      <p>このためマージ後のソースを取得（git clone）した方はこちら 5 に記載の修正は不要です。</p>
      <p>
        <a href="https://github.com/qfgaohao/pytorch-ssd/pull/178" target="_blank">
        Bug fix. by kinoshita-hidetoshi · Pull Request #178 · qfgaohao/pytorch-ssd · GitHub</a></p>
    </div>
  </div>

  <p>&nbsp;</p>
  <p>&nbsp;</p>
  <p>6. 修正したプログラムを動かした様子を以下に示します。</p>
  <p>カメラはPC内蔵のカメラです。PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
  <p>&nbsp;</p>
  
  <p>[動画] プログラムを動作させた様子（PC 内蔵カメラ）</p>
  <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv1-ssd-pccam.mp4" width="800">
    <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
  </video> <br>
    
	<p>&nbsp;</p>
  
  <div class="status_information">
    <div></div>
    <div>
      <p><strong>NOTE</strong></p>
      <p>プログラム "run_ssd_live_demo.py" 中で PC内蔵カメラ からのキャプチャーを行っている部分は下記の箇所です。</p>
      <p><span class="cpp-source">cap = cv2.VideoCapture(0)   # capture from camera</span></p>
    </div>
  </div>

  <p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>

<section>
	<h3><a name="2-2._MobileNetV1-SSD_を動かす（i-PRO_カメラ）">2-2. MobileNetV1-SSD を動かす（i-PRO カメラ）</a></h3>
	<h4>[概要]</h4>
    <p>MobileNetV1-SSD を PyTorch の環境で動かしてみます。</p>
	<p>本章では i-PRO カメラとPCを LAN 接続してリアルタイムで物体検知してみます。前章と同様に <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> を使って行います。</p>
    <p>&nbsp;</p>
	<p> &nbsp;</p>
	
  <h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.11.0+cpu</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

	  <tr>
	    <td>&nbsp;</td>
	    <td>Windows 10 pro,</td>
	    <td>21H2</td>
	  </tr>
		
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
  <p>&nbsp;</p>
  <h4>[手順]</h4>
  <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
  pytorch-ssd</a> から git clone してソースコード一式を入手します。（前章と同じです。実施済みなら不要です。）</p>
	<p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
  <pre style="color: #FFFFFF; background-color: #000000">$ git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
  <p>&nbsp;</p>
	<p>2. "git clone" したフォルダへ移動します。</p>
	<pre style="color: #FFFFFF; background-color: #000000">$ cd pytorch-ssd</pre>
	<p>&nbsp;</p>
  <p>3. README.md 中の "Run the live MobilenetV1 SSD demo" 
  に記載の内容に従って下記を実行します。（前章と同じです。実施済みなら不要です。）</p>
  <p>下記 Google Drive を開いて、"mobilenet-v1-ssd-mp-0_675.pth" および 
  "voc-model-labels.txt" の２つのファイルを取得して models フォルダに保存します。</p>
  <p>
  <a href="https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu" target="_blank">
  https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu</a></p>
  <p>&nbsp;</p>
  <div class="status_information">
    <div>
    </div>
    <div>
      <p><strong>NOTE</strong></p>
      <p>2023/3/11 の 
      <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
      pytorch-ssd</a> 更新によりモデルファイルの取得方法が変更になりました。</p>
    </div>
  </div>
  <p>&nbsp;</p>
  <p>4. 下記コマンドを入力してプログラムを起動します。ここでは４番目の引数として RTSP 表記で i-PRO カメラの接続を記載します。</p>
	<p><span class="auto-style3">{user-id}</span>, <span class="auto-style3">
	{password}</span>, <span class="auto-style3">{ip-address}</span> 
	の部分をご自身が使われる i-PRO カメラの設定に合わせて修正して実行してください。</p>
	<p>その他 RTSP に関しては記事「<a href="connect_with_rtsp.html">RTSP で画像を取得する</a>」を参照ください。</p>
	
  <pre style="color: #FFFFFF; background-color: #000000;">$ python run_ssd_live_demo.py mb1-ssd models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt <span class="auto-style1">rtsp://</span><span class="auto-style6">{user-id}</span>:<span class="auto-style6">{password}</span>@<span class="auto-style6">{ip-address}</span><span class="auto-style1">/MediaInput/stream_1</span></pre>
  
  <p>(例) <span class="cpp-source">python run_ssd_live_demo.py mb1-ssd 
	models/mobilenet-v1-ssd-mp-0_675.pth models/voc-model-labels.txt 
	rtsp://admin:Admin12345@192.168.0.10/MediaInput/stream_1</span></p>
  <p>&nbsp;</p>
	<p>5. プログラムを動かした様子を以下に示します。</p>
	<p>PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
	<p>i-PRO カメラの設定を 10fps にしています。私のノートPC環境では 30fps 
	動作させると映像が少しずつ遅延していきました。AI処理なしに映像表示させると 30fps 
	表示できているので、AI処理に伴うCPU負荷に原因するものと分析します。GPU無しの環境、CPU 版での動作で Full-HD 画像を 10fps 
	でAI処理できているのですから、私個人の見解ですが、 MobileNet は十分に軽量で高性能な AI だと考えます。</p>
  <p>&nbsp;</p>
  
  <p>[動画] プログラムを動作させた様子 １（i-PRO カメラ）</p>
  <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv1-ssd-ipromini_1.mp4" width="800">
    <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
  </video>
  
  <p>&nbsp;</p>
	<p>[動画] プログラムを動作させた様子 ２ － 「<a href="https://pixabay.com/ja/videos/人-商業-店-忙しい-モール-6387/" target="_blank">人 
  商業 店 - Free video on Pixabay</a>」の例（i-PRO カメラ）</p>
  <p>入力画像として、 <a href="https://pixabay.com/" target="_blank">
  https://pixabay.com</a> から取得した下記動画をテストに使用させていただきました。商用利用無料、帰属表示必要なし、のコンテンツです。</p>
  <p>PC上で再生表示する動画を i-PRO カメラで接写しているため、画質が荒いこと、認識精度が微妙なこと、はご容赦ください。</p>

    <p>
	こちらの例の場合、人が一定より小さいと認識できないようでした。カメラをディスプレに近づけて人のサイズを大きくすることで、AIが人を認識できるようになりました。</p>
	<p>別ページ紹介の「<a href="connect_to_wv-xae200w.html">機能拡張ソフトウェア(WV-XAE200W)</a>」でも同じ映像を使って評価していますが、こちらの評価では同じ映像で小さい人を認識できています。この比較からも専用商品である 
	WV-XAE200WUX の画像認識性能の高さを再確認させていただきました。</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv1-ssd-ipromini_2.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video>

    <p>&nbsp;</p>
	<p>&nbsp;</p>
	<p>以上です。というわけで、プログラムを１行も書くことなく i-PRO カメラとPCを接続して MobileNetV1-SSD 
	を実行することができました。</p>
	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>プログラム "run_ssd_live_demo.py" 中で i-PRO カメラからのキャプチャーを行っている部分は下記の箇所です。</p>
		    <p><span class="cpp-source">cap = cv2.VideoCapture(sys.argv[4])  # capture from file</span></p>
		  </div>
    </div>

    <p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<h2><a name="3._MobileNetV2-SSD-Lite">3. MobileNetV2-SSD-Lite</a></h2>
<p>2018年に MobileNet v1 の後継モデルとして v2 が発表されました。（<a href="https://arxiv.org/pdf/1801.04381v3.pdf" target="_blank">MobileNet V2 
の原著論文</a>）</p>
<p>MobileNet v1 
の構想およびモデルを基礎にしながら、モジュールを大幅に改良したものです。</p>
<p>MobileNet v1 
同様、分類・物体検出・セマンティックセグメンテーションを含む画像認識を、モバイル端末などの限られたリソース下で高精度で判別するモデルを作成することを目的として作成しています。</p>
<p>&nbsp;</p>
<p>MobileNetV2 で利用されているモジュールは以下の３つです。本ページでは詳細を割愛します。詳しくは原著論文などを参照ください。</p>
<ol>
  <li><strong>Depthwise Separable Convolutions</strong>（深さ方向分離可能畳み込み）<br>MobileNetV2 でも“Depthwise 
  Separable Convolutions” を利用することで、計算量を減らしています。 </li>
  <li><strong>Linear Bottlenecks</strong>（線形ボトルネック）<br>
  活性関数に非線形性をもつ層（ReLU層やSoftmax層、tanh層など）を利用すると、非常に多くの情報が失われることがあきらかになっています。そのため、ReLUにつながる中間部分のチャンネルを拡大する（＝次元を増やす）ことによって、本来つぶれてしまう情報を他のチャンネルに持たせることができ、情報の喪失を防げるという仮説にもとづき 
  MobileNetV2 では “Bottolneck Convolution” 
  が実装されている。なお、次元を大きくしすぎてもうまく機能しないことが知られており、論文では拡大率(t)を 6 に設定しています。 </li>
  <li><strong>Inverted residuals</strong>（反転残差）<br>ReLUの表現力の問題に関連してInverted residulals 
  blockを考案しています。このblockを基本的に MobileNetV1 の単純な Depthwise Separable 
  Convolution と置き換えます。<br><br>① Inverted residuals block は3つの convolution 
  から構成されています。１つ目は、1×1Conv です。そしてこのConvは t倍(tはthe expantion ratio展開率) 
  に出力チャンネルを写像する役割を持っています。２つ目は、Depthwise Separable Convolutions 
  です。３つ目は、１つ目と同様に1×1convです。こちらは出力チャンネルを入力時のチャンネル次元数に戻すようなConvolutionになっています。 <br>
  <br>② 従来の残差ネットワーク（Residual 
  Network:ResNet）と同様に、ショートカットを使用することで、より高速なトレーニングと精度の向上を可能にしています。 </li>
</ol>
<p>&nbsp;</p>
<p>表現が異なるので単純比較できませんが、V1およびV2のアーキテクチャ構造比較です。類似する場所を同じ色でマーキングしてみました。</p>
<p>V1で「"Conv dw/s1" + "Conv/s1」のセットと 
V2の1階層が対応する、という感じで読むと概ね同じような構造であることを読み取れると思います。</p>
<p><img alt="V2, V1 アーキテクチャ構造" class="border" src="mobilenet-ssd/img2D.jpg"></p>
<p>&nbsp;</p>
<ul>
  <li>上記アーキテクチャ構造について同一インプットサイズのフィルターを V1, V2 
  で比較すると、V2のチャネル数がとても小さくなっていることがわかります。</li>
  <li>例えば、Input サイズ 56x56 の部分を比較してみましょう。（水色の部分）<br>V2 では 56×56×24 
  を3回(n)実施していますが、V1 では 56x56x128 を2回実施しています。少々乱暴ですが、チャネル数で 256(v1) -&gt; 72(v2) 
  と大幅に削減しています。<br>このチャネル数の削減が計算量の削減に大きく貢献しています。</li>
</ul>
<p>&nbsp;</p>
<p>MobileNetV2とMobileNetV1の物体検出性能を、COCOデータセット上で評価・比較した結果です。論文からの引用です。</p>
<p><img alt="MobileNetV2 性能比較" src="mobilenet-ssd/img2F.jpg" class="border"></p>
        
    <table class="border-collapse" border="1" width="500">
      <caption>[Table] 用語説明</caption>
      <thead class="standard_table">
      </thead>
      <tbody>
        <tr>
          <td>mAP</td>
          <td>認識性能の平均値（mean Average Precision）</td>
        </tr>
        <tr>
          <td>Mult-Adds</td>
          <td>積和演算の回数</td>
        </tr>
        <tr>
          <td>Parameters</td>
          <td>重みの数</td>
        </tr>
      </tbody>
    </table>
    
<p>&nbsp;</p>
<p>以上の結果から、MobileNetV2 SSDLite は最も効率的なモデルであるだけでなく、COCOデータセット上で YOLOv2 
を上回る20倍の効率性と10倍の小型化を実現していることを読み取れます。</p>
<p>V1、V2 の比較についても、V2は認識精度（mAP）を低下することなく計算量（MAdd）を約4割削減できていることがわかります。</p>
<p>&nbsp;</p>
<p>引用元： <a href="https://arxiv.org/pdf/1801.04381v3.pdf" target="_blank">MobileNet V2 
の原著論文</a></p>

<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
	<h2> <a name="3-1._MobileNetV2-SSD-Lite_を動かす（PC_内蔵カメラ）">3-1. MobileNetV2-SSD-Lite を動かす（PC 内蔵カメラ）</a></h2>
	<h4>[概要]</h4>
    <p>MobileNetV2-SSD を PyTorch の環境で動かしてみます。</p>
    <p>MobileNetV1-SSD 同様に <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> に含まれていますので、これを取得して動作させてみます。</p>
    <p><a href="mobilenet-ssd/imgC.jpg" target="_blank">
    <img alt="pytorch-ssd ホームページ画面" class="border_with_drop-shadow" src="mobilenet-ssd/imgC.jpg" width="800"></a></p>
    <p>&nbsp;</p>
	<p> &nbsp;</p>
	
  <h4>[評価環境]</h4>
  <table>
    <tbody>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>言語 :</td>
        <td>Python,</td>
        <td>3.10.4 </td>
      </tr>

      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>&nbsp;</td>
        <td>PyTorch,</td>
        <td>1.11.0+cpu</td>
      </tr>

      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>OS :</td>
        <td>Windows 11 home,</td>
        <td>21H2</td>
      </tr>

      <tr>
        <td>&nbsp;</td>
        <td>Windows 10 pro,</td>
        <td>21H2</td>
      </tr>
      
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
    </tbody>
  </table>

  <p>&nbsp;</p>
  <p>&nbsp;</p>
  <h4>[手順]</h4>
  <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
  pytorch-ssd</a> から git clone してソースコード一式を入手します。（前章と同じです。実施済みなら不要です。）</p>
	<p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
  <pre style="color: #FFFFFF; background-color: #000000">$ git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
  <p>&nbsp;</p>
	<p>2. "git clone" したフォルダへ移動します。</p>
	<pre style="color: #FFFFFF; background-color: #000000">$ cd pytorch-ssd</pre>
	<p>&nbsp;</p>
  <p>3. README.md 中の "Run the live MobileNetV2 SSD Lite demo" 
  に記載の内容に従って下記を実行します。</p>
  <p>&nbsp;</p>
  <p>下記 Google Drive を開いて、"mb2-ssd-lite-mp-0_686.pth" および 
  "voc-model-labels.txt" の２つのファイルを取得して models フォルダに保存します。</p>
  <p>
  <a href="https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu" target="_blank">
  https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu</a></p>
  <p>&nbsp;</p>

  <div class="status_information">
    <div>
    </div>
    <div>
      <p><strong>NOTE</strong></p>
      <p>2023/3/11 の 
      <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
      pytorch-ssd</a> 更新によりモデルファイルの取得方法が変更になりました。</p>
    </div>
  </div>

  <p>&nbsp;</p>
  <p>4. 下記コマンドを実行することで推論を実行します。</p>
  
  <pre style="color: #FFFFFF; background-color: #000000; overflow-x: auto;">$ python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt</pre>
  
  <p>&nbsp;</p>
  <p>5. プログラムを動かした様子を以下に示します。</p>
  <p>カメラはPC内蔵のカメラです。PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
  <p>&nbsp;</p>
  
	<p>[動画] プログラムを動作させた様子（PC 内蔵カメラ）</p>
  <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv2-ssd-pccam.mp4" width="800">
    <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
  </video> <br>
    
	<p>&nbsp;</p>
	<p>ちゃんと MobiletNetV2-ssd-lite で動作できていると思いますが、こちら映像を見るだけでは V1, V2 の違いはわからないですね。</p>
	<p>私が触ってみた印象ですが、認識精度が若干向上して CPU 負荷も若干軽くなったという気がします。</p>
	<p>&nbsp;</p>
  
</section>
	
<p>&nbsp;</p>

<section>
	<h3><a name="3-2._MobileNetV2-SSD-Lite_を動かす（i-PRO_カメラ）">3-2. MobileNetV2-SSD-Lite を動かす（i-PRO カメラ）</a></h3>
	<h4>[概要]</h4>
    <p>MobileNetV2-SSD-lite を PyTorch の環境で動かしてみます。</p>
	<p>本章では i-PRO カメラとPCを LAN 接続してリアルタイムで物体検知してみます。前章と同様に <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> を使って行います。</p>
    <p>&nbsp;</p>
	
  <h4>[評価環境]</h4>
  <table>
    <tbody>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>言語 :</td>
        <td>Python,</td>
        <td>3.10.4 </td>
      </tr>

      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>&nbsp;</td>
        <td>PyTorch,</td>
        <td>1.11.0+cpu</td>
      </tr>

      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>

      <tr>
        <td>OS :</td>
        <td>Windows 11 home,</td>
        <td>21H2</td>
      </tr>

      <tr>
        <td>&nbsp;</td>
        <td>Windows 10 pro,</td>
        <td>21H2</td>
      </tr>
      
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
    </tbody>
  </table>

  <p>&nbsp;</p>
  <h4>[手順]</h4>
  <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
  pytorch-ssd</a> から git clone してソースコード一式を入手します。（前章と同じです。実施済みなら不要です。）</p>
	<p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
  <pre style="color: #FFFFFF; background-color: #000000">$ git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
  <p>&nbsp;</p>
	<p>2. "git clone" したフォルダへ移動します。</p>
	<pre style="color: #FFFFFF; background-color: #000000">$ cd pytorch-ssd</pre>
	<p>&nbsp;</p>
  <p>3. README.md 中の "Run the live MobilenetV1 SSD demo" 
  に記載の内容に従って下記を実行します。（前章と同じです。実施済みなら不要です。）</p>
  <p>下記 Google Drive を開いて、"mb2-ssd-lite-mp-0_686.pth" および 
  "voc-model-labels.txt" の２つのファイルを取得して models フォルダに保存します。</p>
  <p>
  <a href="https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu" target="_blank">
  https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu</a></p>
  <p>&nbsp;</p>

  <div class="status_information">
    <div>
    </div>
    <div>
      <p><strong>NOTE</strong></p>
      <p>2023/3/11 の 
      <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
      pytorch-ssd</a> 更新によりモデルファイルの取得方法が変更になりました。</p>
    </div>
  </div>

  <p>&nbsp;</p>
  
	<p>4. 下記コマンドを入力してプログラムを起動します。ここでは４番目の引数として RTSP 表記で i-PRO カメラの接続を記載します。</p>
	<p><span class="auto-style3">{user-id}</span>, <span class="auto-style3">
	{password}</span>, <span class="auto-style3">{ip-address}</span> 
	の部分をご自身が使われる i-PRO カメラの設定に合わせて修正して実行してください。</p>
	<p>その他 RTSP に関しては記事「<a href="connect_with_rtsp.html">RTSP で画像を取得する</a>」を参照ください。</p>
	
  <pre style="color: #FFFFFF; background-color: #000000">$ python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt <span class="auto-style1">rtsp://</span><span class="auto-style6">{user-id}</span>:<span class="auto-style6">{password}</span>@<span class="auto-style6">{ip-address}</span><span class="auto-style1">/MediaInput/stream_1</span></pre>
  
  <p>(例) <span class="cpp-source">python run_ssd_live_demo.py mb2-ssd-lite models/mb2-ssd-lite-mp-0_686.pth models/voc-model-labels.txt 
rtsp://admin:Admin12345@192.168.0.10/MediaInput/stream_1</span></p>
  <p>&nbsp;</p>
    
	<p>5. プログラムを動かした様子を以下に示します。</p>
	<p>PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
	<p>i-PRO カメラの設定を 10fps にしています。私のノートPC環境では 30fps 
	動作させると映像が少しずつ遅延していきました。AI処理なしに映像表示させると 30fps 
	表示できているので、AI処理に伴うCPU負荷に原因するものと分析します。GPU無しの環境、CPU 版での動作で Full-HD 画像を 10fps 
	でAI処理できているのですから、私個人の見解ですが、 MobileNet は十分に軽量で高性能な AI だと考えます。</p>
  <p>&nbsp;</p>
  
  <p>[動画] プログラムを動作させた様子（i-PRO カメラ）</p>
  <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv2-ssd-ipromini_1.mp4" width="800">
    <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
  </video>
  
  <p>&nbsp;</p>
	<p>ちゃんと MobiletNetV2-ssd-lite で動作できていると思いますが、こちら映像を見るだけでは V1, V2 の違いはわからないですね。</p>
	<p>私が触ってみた印象ですが、認識精度が若干向上して CPU 負荷も若干軽くなったという気がします。</p>
	<p>&nbsp;</p>
	<p>&nbsp;</p>
	<p>以上です。というわけで、プログラムを１行も書くことなく i-PRO カメラとPCを接続して MobileNetV2-SSD 
	も実行することができました。</p>
	<p>&nbsp;</p>
</section>

<p>&nbsp;</p>
	
<section>
	<h3><a name="参考：_GPU動作させる場合のソースコード修正について">参考： GPU動作させる場合のソースコード修正について</a></h3>
	<p>本ページの紹介は Pytorch 動作を "CPU" 
	として説明していますが、GPU（CUDA）で動作させた場合に必要となるソースコード修正について参考記載します。</p>
	<p>MobileNetV1 については CUDA 環境で問題なく動作したのですが、MobileNetV2 について私の環境では下記修正を行う必要がありました。</p>
	<p>対象ファイルは "vision/ssd/mobilenet_v2_ssd_lite.py" です。</p>
	<p>&nbsp;</p>
	<p>
	<a href="mobilenet-ssd/imgC1.jpg" target="_blank">
	<img alt="GPU（CUDA）で動作させた場合に必要となるソースコード修正" src="mobilenet-ssd/imgC1.jpg" class="border" width="800"></a></p>
	<p>&nbsp;</p>
  
</section>
	
<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<h2><a name="4._MobileNetV3-SSD-Lite">4. MobileNetV3-SSD-Lite</a></h2>
<p>2019年に MobileNet v2 の後継モデルとして v3 が発表されました。（<a href="https://arxiv.org/pdf/1905.02244.pdf" target="_blank">MobileNet V3 
の原著論文</a>）</p>
<p>MobileNetV3 の要点は下記２つの技術です。本ページでは詳細を割愛します。詳しくは原著論文などを参照ください。</p>
<ol>
  <li>Squeeze-and-Excite</li>
  <li>h-swish</li>
</ol>
<p>&nbsp;</p>
<p>MobileNetV3 は Large と Small の２つのモデルを報告しています。アーキテクチャ構造を以下に示します。</p>
<p><img alt="MobileNetV3 アーキテクチャ構造" class="border" src="mobilenet-ssd/img37.jpg"></p>
<p>&nbsp;</p>
<p>MobileNetV3 の物体検出性能を、COCOデータセット上で評価・比較した結果です。論文からの引用です。</p>
<p><img alt="MobileNetV3 性能比較" src="mobilenet-ssd/img35.jpg"></p>
        
<table class="border-collapse" border="1" width="500">
  <caption>[Table] 用語説明</caption>
  <thead class="standard_table">
  </thead>
  <tbody>
    <tr>
      <td>mAP</td>
      <td>認識性能の平均値（mean Average Precision）</td>
    </tr>
    <tr>
      <td>Mult-Adds</td>
      <td>積和演算の回数</td>
    </tr>
    <tr>
      <td>Parameters</td>
      <td>重みの数</td>
    </tr>
  </tbody>
</table>
    
<p>&nbsp;</p>
<p>上記結果から V3 (Large) は、V1,V2 と同等の認識性能（mAP）を保持しながら、V1 の約半分、V2 
の約７割の演算量（MAdds）という軽量なモデルを実現したと読み取れます。</p>
<p>V3-Small は Large のさらに半分の演算量を実現してますが認識性能は相応に低下するようです。 </p>
<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
<h2><a name="4-1._ssdlite320_mobilenet_v3_large">4-1. ssdlite320_mobilenet_v3_large</a></h2>
<p>MobileNetV3-SSD-Lite は PyTorch に組み込まれていました。"ssdlite320_mobilenet_v3_large" 
というモジュールです。同様に学習済みモデルも Pytorch 環境のみで取得可能です。</p>
<p>本章では Pytorch および "ssdlite320_mobilenet_v3_large" を使って MobileNetV3-SSD-Lite 
を動かしてみたいと思います。</p>
<p>（"ssdlite320_mobilenet_v3_large" のインプットサイズはファイル名記載の通り 320x320 
にアレンジされているように見えます。詳細は割愛します。）</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
	<h2> <a name="4-1-1._MobileNetV3-SSD-Lite_を動かす（JPEGファイル）">4-1-1. MobileNetV3-SSD-Lite を動かす（JPEGファイル）</a></h2>
	<h4>[概要]</h4>

	<p>MobileNetV3-SSD-Lite は PyTorch に組み込まれていました。"ssdlite320_mobilenet_v3_large" 
	というモジュールです。同様に学習済みモデルも Pytorch 環境のみで取得可能です。</p>
    <p>本節では Pytorch および "ssdlite320_mobilenet_v3_large" を使って 
	MobileNetV3-SSD-Lite を動かしてみたいと思います。</p>
	<p>（"ssdlite320_mobilenet_v3_large" のインプットサイズはファイル名記載の通り 320x320 
	にアレンジされているように見えます。詳細は割愛します。）</p>
	<p>&nbsp;</p>
	<p>PyTorch の資料を参考に、JPEGファイルを読み込んで物体認識する Python プログラムを作成してみます。</p>
	<p>参考にする元資料は 
	<a href="https://pytorch.org/vision/main/models.html#object-detection" target="_blank">
	こちら</a> (Object Detection) です。<br>この元資料の "fasterrcnn_resnet50_fpn_v2" を "<strong>ssdlite320_mobilenet_v3_large</strong>" 
	へ変更して、課題ある部分をさらに追加修正する、という感じでやってみます。</p>
	<p>学習済みモデルは COCO dataset を使って学習しているので、このプログラムは下記 91クラス を検知します。</p>
	<p>このプログラムを初めて実行するとき、このプログラムは学習済みモデルのダウンロードを行うため、プログラム起動に多くの時間を待つ必要があるでしょう。 </p>
	<p>&nbsp;</p>
	<pre style="width: 200px">1: person
2: bicycle
3: car
4: motorcycle
5: airplane
6: bus
7: train
8: truck
9: boat
10: traffic light
11: fire hydrant
12: street sign*
13: stop sign
14: parking meter
15: bench
16: bird
17: cat
18: dog
19: horse
20: sheep
21: cow
22: elephant
23: bear
24: zebra
25: giraffe
26: hat
27: backpack
28: umbrella
29: shoe
30: eye glasses
31: handbag
32: tie
33: suitcase
34: frisbee
35: skis
36: snowboard
37: sports ball
38: kite
39: baseball bat
40: baseball glove
41: skateboard
42: surfboard
43: tennis racket
44: bottle
45: plate
46: wine glass
47: cup
48: fork
49: knife
50: spoon
51: bowl
52: banana
53: apple
54: sandwich
55: orange
56: broccoli
57: carrot
58: hot dog
59: pizza
60: donut
61: cake
62: chair
63: couch
64: potted plant
65: bed
66: mirror
67: dining table
68: window
69: desk
70: toilet
71: door
72: tv
73: laptop
74: mouse
75: remote
76: keyboard
77: cell phone
78: microwave
79: oven
80: toaster
81: sink
82: refrigerator
83: blender
84: book
85: clock
86: vase
87: scissors
88: teddy bear
89: hair drier
90: toothbrush
91: hair brush</pre>
	<p>&nbsp;</p>
	<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
	&nbsp;</p>
	<h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1+cpu</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

	  <tr>
	    <td>&nbsp;</td>
	    <td>Windows 10 pro,</td>
	    <td>21H2</td>
	  </tr>
		
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
  
  <p>&nbsp;</p>
	<p>["<a href="https://github.com/i-pro-corp/python-examples/blob/main/mobilenet-ssd/run_mobilenetv3-ssdlite_jpeg_demo.py" target="_blank">run_mobilenetv3-ssdlite_jpeg_demo.py</a>"]</p>
	<pre class="prettyprint linenums lang-py">'''
[Abstract]
    Create a python program that detects objects in JPEG images using the trained AI model of "MobileNetV3 SSD-Lite".

    "MobileNetV3 SSD-Lite" の学習済みAIモデルを使用して、JPEG画像を対象に物体検知する Python プログラムを作成します。

[Details]
    This program uses pytorch "ssdlite320_mobilenet_v3_large" and pre-trained model.
    The program detects 91 classes because the trained model is trained using the COCO dataset.
    When you run this program for the first time, you will have to wait a lot of time to start the program, as it downloads the trained model.

    このプログラムは pytorch の "ssdlite320_mobilenet_v3_large" および 学習済みモデルを使用します。
    学習済みモデルは COCO dataset を使って学習しているので、このプログラムは91クラスを検知します。
    このプログラムを初めて実行するとき、このプログラムは学習済みモデルのダウンロードを行うため、あなたはプログラム起動に多くの時間を待つ必要があるでしょう。

[Library install]
    cv2:        pip install opencv-python
    pytorch:    pip install torch torchvision torchaudio

[Reference URL]
    https://pytorch.org/vision/main/models.html#object-detection
    https://pytorch.org/vision/0.11/auto_examples/plot_visualization_utils.html#visualizing-bounding-boxes
'''

from pathlib import Path
from torchvision.io.image import read_image
from torchvision.models.detection import <span class="auto-style1">ssdlite320_mobilenet_v3_large</span>, <span class="auto-style1">SSDLite320_MobileNet_V3_Large_Weights</span>
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import to_pil_image

img = read_image(str(Path('assets') / 'dog1.jpg'))

# Step 1: Initialize model with the best available weights
weights = <span class="auto-style1">SSDLite320_MobileNet_V3_Large_Weights</span>.DEFAULT
model = <span class="auto-style1">ssdlite320_mobilenet_v3_large</span>(weights=weights)
model = model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Step 3: Apply inference preprocessing transforms
batch = [preprocess(img)]

# Step 4: Use the model and visualize the prediction
prediction = model(batch)[0]
score_threshold = 0.5
labels = [weights.meta["categories"][class_index] + f": {score_int:.2f}" for class_index, score_int in zip(prediction["labels"], prediction["scores"]) if score_int > score_threshold]
boxes = prediction["boxes"][prediction["scores"] > score_threshold]
box = draw_bounding_boxes(img, boxes=boxes,
                          labels=labels,
                          colors="red",
                          width=4,
                          font='arial.ttf', font_size=30)
im = to_pil_image(box.detach())
im.show()</pre>
	<p>&nbsp;</p>
	<p>
	たったこれだけのソースコードで物体検知を実現できます。学習済みモデルのダウンロードを含みますので、必ずインターネットへ接続できる環境で実行してください。</p>
	<p>&nbsp;</p>
	<p>動作結果はこちらです。</p>
	<p>	評価画像として、&nbsp;<a href="https://pixabay.com" target="_blank">https://pixabay.com</a>&nbsp;提供の画像を使用させていただきました。商用利用無料、帰属表示必要なし、の画像です。</p>
  <p><a href="https://pixabay.com/ja/photos/マルタ語-犬-子犬-小型犬-1123016/" target="_blank">
    https://pixabay.com/ja/photos/マルタ語-犬-子犬-小型犬-1123016/</a></p>
  <p>
    <strong>[注意]</strong>
    上記プログラムを実行する際は、ご自身で画像を準備またはダウンロードして事前に "assets" フォルダに "dog1.jpg" 
    の名称で保存してください。</p>
	<p><img alt="MobileNetV3 静止画による物体検知 実施例" src="mobilenet-ssd/img8.jpg" width="800"></p>
    
</section>
	
<p>&nbsp;</p>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
学習済みモデルは下記に保存されます。'~' はログインしているユーザーのホームディレクトリを意味します。</p>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
&nbsp;</p>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
<a href="https://pytorch.org/docs/stable/hub.html#where-are-my-downloaded-models-saved" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px; background-color: transparent;" target="_blank">
Where are my downloaded models saved?</a></p>
<blockquote style="background-repeat: no-repeat; box-sizing: inherit; margin: 2em 0px; padding: 1em; background-color: rgb(250, 250, 250); color: rgb(85, 85, 85); border-left: 8px solid rgb(224, 224, 224); font-size: 13.3px; display: block; margin-block: 1em; font-family: sans-serif; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
  <p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif;">
  The locations are used in the order of</p>
  <ul style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 5px 0px 5px 20px; font-family: sans-serif; list-style-position: outside; list-style-type: disc;">
	<li style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px;">
	Calling<span>&nbsp;</span><span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">hub.set_dir(&lt;PATH_TO_HUB_DIR&gt;)</span></li>
	<li style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px;">
	<span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">
	$TORCH_HOME/hub</span>, if environment variable<span>&nbsp;</span><span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">TORCH_HOME</span><span>&nbsp;</span>is 
	set.</li>
	<li style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px;">
	<span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">
	$XDG_CACHE_HOME/torch/hub</span>, if environment variable<span>&nbsp;</span><span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">XDG_CACHE_HOME</span><span>&nbsp;</span>is 
	set.</li>
	<li style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px; padding: 0px;">
	<span class="cpp-source" style="background-repeat: no-repeat; box-sizing: inherit; margin: 0px 1px !important; padding: 0px 1px !important; display: inline; line-height: 1.42857; color: rgb(0, 0, 204); background-color: rgb(245, 245, 245); border: 1px solid rgb(204, 204, 204); border-radius: 4px;">
	~/.cache/torch/hub</span></li>
  </ul>
</blockquote>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
私の場合は下記に ssdlite320_mobilenet_v3_large_coco-a79551df.pth 
というファイルを保存していました。約13MBのファイルサイズでした。</p>
<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
"~\.cache\torch\hub\checkpoints\ssdlite320_mobilenet_v3_large_coco-a79551df.pth"</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>

<section>
	<h2> <a name="4-1-2._MobileNetV3-SSD-Lite_を動かす（PC_内蔵カメラ）">4-1-2. MobileNetV3-SSD-Lite を動かす（PC 内蔵カメラ）</a></h2>
	<h4>[概要]</h4>
	<p>4-1 で作成した JPEG 
	画像を対象に物体検知するプログラムを元に、ここではPC内蔵カメラの映像をライブで物体検知するプログラムを作成してみます。</p>
	<p>PC内蔵カメラからの映像取得は OpenCV を使って行います。</p>
	<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
	&nbsp;</p>
	<h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1+cpu</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

	  <tr>
	    <td>&nbsp;</td>
	    <td>Windows 10 pro,</td>
	    <td>21H2</td>
	  </tr>
		
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    
    <div class="status_ok">
      <div></div>
      <div>
        <p><strong>ポイント</strong></p>
        <p>OpenCV 形式で取得した画像を PyTorch が扱える形式へ変換する必要があります。</p>
		<ul>
		  <li>OpenCVの画像は BGR の並び順になっています。BGR を RGB へ並び替えます。</li>
		  <li>&nbsp;データの並び順を [width][height][channel] から 
		  [channel][width][height] へ変換します。</li>
		  <li>各画素のデータ形式を整数(0-255)から浮動小数点(0.0-1.0)へ変換します。</li>
		</ul>
      </div>
    </div>

    <p>&nbsp;</p>
  
	<p>["<a href="https://github.com/i-pro-corp/python-examples/blob/main/mobilenet-ssd/run_mobilenetv3-ssdlite_live_pc-cam_demo.py" target="_blank">run_mobilenetv3-ssdlite_live_pc-cam_demo.py</a>"]</p>
	<pre class="prettyprint linenums lang-py">'''
[Abstract]
    Create a python program to detect objects in the camera live video using the trained AI model of "MobileNetV3 SSD-Lite".

    "MobileNetV3 SSD-Lite" の学習済みAIモデルを使用して、カメラライブ映像を物体検知する Python プログラムを作成してみます。

[Details]
    This program uses pytorch "ssdlite320_mobilenet_v3_large" and pre-trained model.
    The program detects 91 classes because the trained model is trained using the COCO dataset.
    When you run this program for the first time, you will have to wait a lot of time to start the program, as it downloads the trained model.

    このプログラムは pytorch の "ssdlite320_mobilenet_v3_large" および 学習済みモデルを使用します。
    学習済みモデルは COCO dataset を使って学習しているので、このプログラムは91クラスを検知します。
    このプログラムを始めて実行するとき、このプログラムは学習済みモデルのダウンロードを行うため、あなたはプログラム起動に多くの時間を待つ必要があるでしょう。

[Library install]
    cv2:        pip install opencv-python
    pytorch:    pip install torch torchvision torchaudio
'''

import cv2
import torch
from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights
from torchvision.transforms.functional import convert_image_dtype
from torchvision import transforms


# Step 1: Initialize model with the best available weights
weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT
model = ssdlite320_mobilenet_v3_large(weights=weights)
model = model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Initialize variables.
cap = cv2.VideoCapture(0)       # Capture from camera.
#cap.set(3, 1920)               # Set video stream frame width.  Remove '#' and change the value according to your needs.
#cap.set(4, 1080)               # Set video stream frame height. Remove '#' and change the value according to your needs.
winname = "Annotated"           # Window title.

# Exception definition.
BackendError = type('BackendError', (Exception,), {})


def IsWindowVisible(winname):
    '''
    [Abstract]
        Check if the target window exists.
        対象ウィンドウが存在するかを確認する。
    [Param]
        winname :       Window title
    [Return]
        True :          exist
                        存在する
        False :         not exist
                        存在しない
    [Exception]
        BackendError :
    '''
    try:
        ret = cv2.getWindowProperty(winname, cv2.WND_PROP_VISIBLE)
        if ret == -1:
            raise BackendError('Use Qt as backend to check whether window is visible or not.')

        return bool(ret)

    except cv2.error:
        return False


while True:
    # Capture image by opencv.
    ret, orig_image = cap.read()
    if orig_image is None:
        continue

    # Convert image from BGR to RGB.
    rgb_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)

    # Convert image from numpy.ndarray to torchvision image format.
    rgb_image = rgb_image.transpose((2, 0, 1))
    rgb_image = rgb_image / 255.0
    rgb_image = torch.FloatTensor(rgb_image)

    # Step 3: Apply inference preprocessing transforms
    batch = [preprocess(rgb_image)]

    # Step 4: Use the model and visualize the prediction
    with torch.no_grad():
        prediction = model(batch)[0]

    score_threshold = 0.5
    labels = [weights.meta["categories"][class_index] + f": {score_int:.2f}" for class_index, score_int in zip(prediction["labels"], prediction["scores"]) if score_int &gt; score_threshold]
    boxes = prediction["boxes"][prediction["scores"] &gt; score_threshold]

    # Draw result.
    for box, label in zip(boxes, labels):

        cv2.rectangle(
            orig_image,                         # Image.
            (int(box[0]), int(box[1])),         # Vertex of the rectangle.
            (int(box[2]), int(box[3])),         # Vertex of the rectangle opposite to pt1.
            (255, 255, 0),                      # Color.
            4 )                                 # Line type.

        cv2.putText(
            orig_image,                         # Image.
            label,                              # Text string to drawn.
            (int(box[0])+20, int(box[1])+40),   # Bottom-left corner of the text string in the image.
            cv2.FONT_HERSHEY_SIMPLEX,           # Font face. - フォント種別
            0.8,                                # Font scale.
            (255, 0, 255),                      # Color.
            2)                                  # Line type.

    # Display video.
    cv2.imshow(winname, orig_image)

    # Press the "q" key to finish.
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

    # Exit the program if there is no specified window.
    if not IsWindowVisible(winname):
        break

cap.release()
cv2.destroyAllWindows()
</pre>
	<p>&nbsp;</p>
    
  	<p>[動画] プログラムを動作させた様子（PC 内蔵カメラ）</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv3-ssd-pccam.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video> <br>
    
	<p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>

<section>
	<h2> <a name="4-1-3._MobileNetV3-SSD-Lite_を動かす（i-PRO_内蔵カメラ）">4-1-3. MobileNetV3-SSD-Lite を動かす（i-PRO カメラ）</a></h2>
	<h4>[概要]</h4>
	<p>4-1-2 で作成したプログラムを元に i-PRO カメラと接続してカメラ映像をライブで物体検知するプログラムを作成してみます。</p>
	<p>cv2.VideoCapture の引数を "0" から RTSP表記 へ変更するだけです。</p>
	<p style="background-repeat: no-repeat; box-sizing: inherit; margin: 6px 0px; padding: 0px; color: rgb(0, 0, 0); font-family: sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;">
	&nbsp;</p>
	<h4>[評価環境]</h4>
  <table>
  <tbody>
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>言語 :</td>
      <td>Python,</td>
      <td>3.10.4 </td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>&nbsp;</td>
      <td>PyTorch,</td>
      <td>1.12.1+cpu</td>
    </tr>

    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>

    <tr>
      <td>OS :</td>
      <td>Windows 11 home,</td>
      <td>21H2</td>
    </tr>

	  <tr>
	    <td>&nbsp;</td>
	    <td>Windows 10 pro,</td>
	    <td>21H2</td>
	  </tr>
		
    <tr>
      <td class="td_separate" colspan="3"></td>
    </tr>
  </tbody>
</table>

  <p>&nbsp;</p>
    
    <div class="status_ok">
      <div></div>
      <div>
        <p><strong>ポイント</strong></p>
        <p><strong>cv2.VideoCapture</strong> 関数の引数として <strong>RTSP</strong> 
		と呼ばれる表記でネットワークカメラとの接続情報を渡します。</p>
		<p>この表記方法はネットワークカメラによって異なります。下記サンプルソースコードの表記は 
		i-PRO カメラで使用されている表記です。</p>
      </div>
    </div>

    <p>&nbsp;</p>
	<p>"user-id", "password", "host" 
	の部分をご自身が使われる i-PRO カメラの設定に合わせて修正してください。</p>
	<p>その他 RTSP に関しては記事「<a href="connect_with_rtsp.html">RTSP で画像を取得する</a>」を参照ください。</p>
  
  <p>&nbsp;</p>
	<p>["<a href="https://github.com/i-pro-corp/python-examples/blob/main/mobilenet-ssd/run_mobilenetv3-ssdlite_live_i-pro-cam_demo.py" target="_blank">run_mobilenetv3-ssdlite_live_i-pro-com_demo.py</a>"]</p>
	<pre class="prettyprint linenums lang-py">'''
[Abstract]
    Create a python program to detect objects in the camera live video using the trained AI model of "MobileNetV3 SSD-Lite".

    "MobileNetV3 SSD-Lite" の学習済みAIモデルを使用して、カメラライブ映像を物体検知する Python プログラムを作成してみます。

[Details]
    This program uses pytorch "ssdlite320_mobilenet_v3_large" and pre-trained model.
    The program detects 91 classes because the trained model is trained using the COCO dataset.
    When you run this program for the first time, you will have to wait a lot of time to start the program, as it downloads the trained model.

    このプログラムは pytorch の "ssdlite320_mobilenet_v3_large" および 学習済みモデルを使用します。
    学習済みモデルは COCO dataset を使って学習しているので、このプログラムは91クラスを検知します。
    このプログラムを始めて実行するとき、このプログラムは学習済みモデルのダウンロードを行うため、あなたはプログラム起動に多くの時間を待つ必要があるでしょう。

[Library install]
    cv2:        pip install opencv-python
    pytorch:    pip install torch torchvision torchaudio
'''

import cv2
import torch
from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights
from torchvision.transforms.functional import convert_image_dtype
from torchvision import transforms


# Initialize variables.
<span class="auto-style1">user_id     = "user-id"             # Change to match your camera setting</span>
<span class="auto-style1">user_pw     = "password"            # Change to match your camera setting</span>
<span class="auto-style1">host        = "192.168.0.10"        # Change to match your camera setting</span>
<span class="auto-style1">resolution  = "1920x1080"           # Resolution</span>
<span class="auto-style1">framerate   =  5                    # Framerate</span>
winname     = "Annotated"           # Window title.

# Step 1: Initialize model with the best available weights
weights = SSDLite320_MobileNet_V3_Large_Weights.DEFAULT
model = ssdlite320_mobilenet_v3_large(weights=weights)
model = model.eval()

# Step 2: Initialize the inference transforms
preprocess = weights.transforms()

# Capture from camera.
<span class="auto-style1">url = f"rtsp://{user_id}:{user_pw}@{host}/MediaInput/stream_1"  # H.264/H.265</span>
<span class="auto-style1">#url = f"http://{user_id}:{user_pw}@{host}/cgi-bin/nphMotionJpeg?Resolution={resolution}&amp;Quality=Standard&amp;Framerate={framerate}"    # MJPEG</span>
<span class="auto-style1">cap = cv2.VideoCapture(url)</span>

# Exception definition.
BackendError = type('BackendError', (Exception,), {})


def IsWindowVisible(winname):
    '''
    [Abstract]
        Check if the target window exists.
        対象ウィンドウが存在するかを確認する。
    [Param]
        winname :       Window title
    [Return]
        True :          exist
                        存在する
        False :         not exist
                        存在しない
    [Exception]
        BackendError :
    '''
    try:
        ret = cv2.getWindowProperty(winname, cv2.WND_PROP_VISIBLE)
        if ret == -1:
            raise BackendError('Use Qt as backend to check whether window is visible or not.')

        return bool(ret)

    except cv2.error:
        return False


while True:
    # Capture image by opencv.
    ret, orig_image = cap.read()
    if orig_image is None:
        continue

    # Convert image from BGR to RGB.
    rgb_image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)

    # Convert image from numpy.ndarray to torchvision image format.
    rgb_image = rgb_image.transpose((2, 0, 1))
    rgb_image = rgb_image / 255.0
    rgb_image = torch.FloatTensor(rgb_image)

    # Step 3: Apply inference preprocessing transforms
    batch = [preprocess(rgb_image)]

    # Step 4: Use the model and visualize the prediction
    with torch.no_grad():
        prediction = model(batch)[0]

    score_threshold = 0.5
    labels = [weights.meta["categories"][class_index] + f": {score_int:.2f}" for class_index, score_int in zip(prediction["labels"], prediction["scores"]) if score_int &gt; score_threshold]
    boxes = prediction["boxes"][prediction["scores"] &gt; score_threshold]

    # Draw result.
    for box, label in zip(boxes, labels):

        cv2.rectangle(
            orig_image,                         # Image.
            (int(box[0]), int(box[1])),         # Vertex of the rectangle.
            (int(box[2]), int(box[3])),         # Vertex of the rectangle opposite to pt1.
            (255, 255, 0),                      # Color.
            4 )                                 # Line type.

        cv2.putText(
            orig_image,                         # Image.
            label,                              # Text string to drawn.
            (int(box[0])+20, int(box[1])+40),   # Bottom-left corner of the text string in the image.
            cv2.FONT_HERSHEY_SIMPLEX,           # Font face. - フォント種別
            0.8,                                # Font scale.
            (255, 0, 255),                      # Color.
            2)                                  # Line type.

    # Display video.
    cv2.imshow(winname, orig_image)

    # Press the "q" key to finish.
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        break

    # Exit the program if there is no specified window.
    if not IsWindowVisible(winname):
        break

cap.release()
cv2.destroyAllWindows()</pre>
  	<p>&nbsp;</p>
      
   	<p>[動画] プログラムを動作させた様子（i-PRO カメラ、5fps）</p>
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv3-ssd-ipromini_1.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video> 
      
  	<p>&nbsp;</p>
  	
    <video controls muted autoplay="y" loop="y" src="mobilenet-ssd/mobilenetv3-ssd-ipromini_2.mp4" width="800">
      <p>動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。</p>
    </video>
  
  	<p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>軽いとはいえ MobileNetV3 
		もそれなりにCPUを使用します。あなたの環境に合わせてカメラ映像のフレームレートを調節してください。多くの場合で 10fps または 5fps 
		ぐらいに設定する必要があると思います。</p>
        <p>物体検知を別プロセスで処理する（<a href="connect_with_rtsp.html#4-2._%E9%A1%94%E6%A4%9C%E7%9F%A5%E9%83%A8%E5%88%86%E3%82%92%E5%88%A5%E3%83%97%E3%83%AD%E3%82%BB%E3%82%B9%E3%81%AE%E5%87%A6%E7%90%86%E3%81%AB%E3%81%97%E3%81%A6%E3%81%BF%E3%82%8B">参考記事</a>）ことで映像をフルレート表示するという方法もあります。よろしければ試してみてください。</p>
      </div>
    </div>

    <p>&nbsp;</p>
  
    <div class="status_information">
      <div></div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>私の環境では V3 より V1、V2 の方が動作が軽い印象です。V1、V2 
		サンプルは10fpsで動作しましたが、V3 は5fpsまでカメラ設定を下げる必要がありました。</p>
        <p>
		V1、V2サンプルはVOCデータセットでクラス数が20です。一方、V3サンプルはCOCOデータセットでクラス数が91という違いがあります。<br>
		加えて、V1、V2サンプルの入力画像サイズは 300x300 ですが、V3サンプルは 320x320 という違いもあります。そもそもベースとなっているプログラムが異なります。</p>
		<p>以上の理由から、V1、V2、V3 を適正に比較評価できているわけでないこと、ご承知ください。ちゃんと同じ条件で比較したら V3 
		がやはりもっとも高性能という結果になるのではと期待しています。（クラス数の比が演算数にそのまま影響あるとすると概ね妥当な結果かな、と思っています。）</p>
      </div>
    </div>

    <p>&nbsp;</p>
    
</section>
	
<p>&nbsp;</p>
	
<section>
	<h3>参考： GPU動作について</h3>
	<p>本ページの紹介は Pytorch 動作を "CPU" 
	として説明していますが、私の環境で GPU（CUDA）で動作させた場合の概況について参考記載します。</p>
	<p>上記ソースコードから device を "cuda:0" へ変更するためのソースコード修正を行うことで、私の環境では特にエラーなど発生することなく 
	CUDA 環境で動作させることができました。しかし残念ながら CPU 
	動作からパフォーマンス向上を確認することができませんでした。私の実現方法に課題があるのか、または PyTorch 
	側に問題があるのか、問題の切り分けをまだできておりません。</p>
	<p>進展あればこちらへ追記していきたいと思います。</p>
	<p>&nbsp;</p>
  
</section>
</section>

<p>&nbsp;</p>

<section>
  <h2><a name="4-2._MobileNetV3-large-SSD-Lite">4-2. MobileNetV3-large-SSD-Lite</a></h2>
  <p>MobilenetV3 は 
  <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">pytorch-ssd</a> 
  に含まれていますが、学習済みモデルを提供していませんでした。残念。</p>
  <p>
  折角なので、私自身により学習を実施してみました。皆さんが期待するほどの認識性能に達していないかもしれませんが、本ページから私が作成した学習済みデータをダウンロードできるようにしました。このデータを使って 
  MobileNet-SSD-V3 による推論を実行してみたいと思います。合わせてこの学習データを作成した手順についても参考記載します。</p>
  <p>※ これで V1,V2,V3 を同等条件で動作比較できそうです。</p>
  <p>&nbsp;</p>

  <section>
    <h3><a name="4-2-1._MobileNetV3-large-SSD-Lite_学習データを作る">4-2-1. 
    MobileNetV3-large-SSD-Lite 学習データを作る</a></h3>
    <h4>[概要]</h4>
    <p>"MobileNetV3-large-ssd-lite" の学習データを "VOC dataset" により作成してみます。</p>
    <p>学習データ作成なので、本ページの当初ポリシーから外れますが、ここでは GPU を使用させていただきます。</p>
    <p>&nbsp;</p>
    <h4>[評価環境]</h4>
    <table>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>言語 :</td>
        <td>Python,</td>
        <td>3.10.7</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td></td>
        <td>PyTorch,</td>
        <td>1.12.1+cu116</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>OS :</td>
        <td>Windows 11 home,</td>
        <td>22H2</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>GPU：</td>
        <td>NVIDIA GeForce GTX 1650,</td>
        <td>GPUメモリ 4GB</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
    </table>
    <p>&nbsp;</p>
    <h4>[手順]</h4>
    <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> から git clone してソースコード一式を入手します。（前章と同じです。実施済みなら不要です。）</p>
    <p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
    <p>&nbsp;</p>
    <p>2. "git clone" したフォルダへ移動します。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ cd pytorch-ssd</pre>
    <p>&nbsp;</p>
    <p>3. PASCAL VOC2007/2012 のデータセットをダウンロード、展開します。</p>
    <p>■Linux の場合</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ cd data
$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
$ wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar
$ tar -xvf VOCtrainval_11-May-2012.tar&nbsp;
$ tar -xvf VOCtrainval_06-Nov-2007.tar&nbsp;
$ tar -xvf VOCtest_06-Nov-2007.tar&nbsp;</pre>
    <p>&nbsp;</p>
    <p>■Windows の場合</p>
    <p>&nbsp;Windows に標準では wget コマンドは無いため上記を実行してもエラーになります。ここでは、wget の代わりに bitsadmin 
    コマンドで代用する方法を紹介します。</p>
    <p>&nbsp;</p>
    <p class="auto-style2"><strong>bitsadmin.exe</strong> の書き方：</p>
    <blockquote>
      <strong>bitsadmin.exe</strong> /transfer ＜ジョブ名＞ ＜URL＞ ＜保存先ファイル名（フルパス）＞ 
    </blockquote>
    <p>上記内容を bitsadmin.exe で置き換えると以下のようになるので、Windows 環境の方はこちらで実施します。<br>
    <span class="auto-style3">c:\{作業フォルダ}</span> の部分をご自身の環境に合わせて修正して実行してください。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ cd data
$ bitsadmin /TRANSFER htmldl http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar <span class="auto-style3">c:\{作業フォルダ}</span>\data\VOCtrainval_11-May-2012.tar
$ bitsadmin /TRANSFER htmldl http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar <span class="auto-style3">c:\{作業フォルダ}</span>\data\VOCtrainval_06-Nov-2007.tar
$ bitsadmin /TRANSFER htmldl http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar <span class="auto-style3">c:\{作業フォルダ}</span>\data\VOCtest_06-Nov-2007.tar
$ tar -xvf VOCtrainval_11-May-2012.tar&nbsp;
$ tar -xvf VOCtrainval_06-Nov-2007.tar&nbsp;
$ tar -xvf VOCtest_06-Nov-2007.tar&nbsp;</pre>
    <p>&nbsp;</p>
    <p>以上を実行すると、"data" フォルダの中に "VOCdevkit" というフォルダが作成されて、その中に "VOC2007" と 
    "VOC2012" の２つのフォルダと大量のデータが作成されます。</p>
    <p>&nbsp;</p>
    <p>4. データセットを再配置します。以下の順に実施します。</p>
    <p>(1) "data" フォルダ中に "VOC0712" フォルダを作成し、この中に "VOC2007" と "VOC2012" 
    を全てコピーします。</p>
    <p>(2) "VOC0712" フォルダ中に "test" フォルダを作成し、このなかに "VOC2007" をコピーします。</p>
    <p>&nbsp;</p>
    <p>5. 学習を実行します。私の場合は下記コマンドを実行しました。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ cd ..
$ python train_ssd.py  --dataset_type voc  --datasets ./data/VOC0712/VOC2007 ./data/VOC0712/VOC2012 --validation_dataset ./data/VOC0712/test/VOC2007 --net mb3-large-ssd-lite  --scheduler cosine --lr 0.01 --t_max 200 --validation_epoch 5 --num_epochs 200 --batch_size 16
</pre>
    <p>&nbsp;</p>
    <ul>
      <li>"README.md" 中の "<a href="https://github.com/qfgaohao/pytorch-ssd#training" target="_blank">Training</a>" 
      の内容を元に微修正しました。</li>
      <li>"--net" を "mb3-large-ssd-lite" に設定しました。</li>
      <li>"--base_net" として使用できるモデルが無かったため無指定です。</li>
      <li>"--batch_size" を "16" に設定しました。使用したGPUのRAMが4GBと小さめであるため、16 
      より大きいサイズを指定するとメモリ不足でエラーになりました。8GB など大きなメモリを持つGPUを所有している場合は "24" や "32" 
      など大きな値を設定することでより良好に学習を進めることができるかもしれません。</li>
    </ul>
    
    <p>&nbsp;</p>
    
    <div class="status_warning">
      <div></div>
      <div>
        <p><strong>注意</strong></p>
        <p>Windows 環境でこの学習を行う場合、「<a href="mobilenet-ssd_train.html#5-1-2._学習">5-1-2. 学習</a>」に記載の内容に従って 
        "train_ssd.py" および "vision/ssd/data_preprocessing.py" を修正する必要があります。</p>
      </div>
    </div>
    
    <p>&nbsp;</p>
    
    <p>6. (参考) 中断してしまった学習を継続する方法</p>
    <p>何かしらの理由でPCの電源をOffする必要があった、など学習を中断してしまった場合、下記コマンドを参考に前回学習結果を 
    "--pretraind_ssd" 
    として指定することである程度続きから学習を再開することができそうです。ただし完全な継続にはなりません。ある程度の学習回数後に概ね前回の状態に戻る感じです。私の感覚では 
    Epoch で20回ぐらいかかりました。それでも全部やり直しすることと比較すればとても有効な手段だと思います。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ python train_ssd.py  --dataset_type voc  --datasets ./data/VOC0712/VOC2007 ./data/VOC0712/VOC2012 --validation_dataset ./data/VOC0712/test/VOC2007 --net mb3-large-ssd-lite  <span class="auto-style3"><strong>--pretrained_ssd  models/mb3-large-ssd-lite-Epoch-NNN-Loss-N.NNNNNNNN.pth</strong></span>  --scheduler cosine  <span class="auto-style3"><strong>--lr 0.001</strong></span>  --t_max 200 --validation_epoch 5 --num_epochs 200 --batch_size 16</pre>
    <p>&nbsp;</p>
    <ul>
      <li>"--pretrained_ssd" に設定する 
      "mb3-large-ssd-lite-Epoch-NNN-Loss-N.NNNNNNNN.pth" 
      はあなたが前回中断した学習途中で生成した学習済みデータのファイル名を記載します。Loss の値が最も小さいファイルを選択してください。</li>
      <li>"--lr" の値も前回の進捗に合わせて小さい値へ変更することが望ましいです。例えば、前回 200 Epoch 
      中の100回ぐらいで中断した場合、きっと学習速度は半分ぐらいに減速されているでしょうから、初期値 0.01 の半分ぐらいの値として "--lr 
      0.005" などを設定します。</li>
    </ul>
    <p>&nbsp;</p>
    <p>あとは学習が期待通りに進むのを期待して待ちます。</p>
    <p>私が作成した学習済みデータを "<a href="mobilenet-ssd/mb3-large-ssd-lite-mp-0_652.pth">こちら</a>" 
    から取得可能です。<br>この学習済みデータの認識性能は以下の通りです。"eval_ssd.py" により測定した結果です。</p>
    <p>&nbsp;</p>
    <p>Model: mb3-large-ssd-lite-mp-0_652.pth</p>
    <pre>Average Precision Per-class:
aeroplane: 0.6858618154005621
bicycle: 0.7953144174658359
bird: 0.5944325029442169
boat: 0.5254286014580958
bottle: 0.29775074049868244
bus: 0.7695870498857185
car: 0.7172673114401424
cat: 0.8129718499191528
chair: 0.4989150543653392
cow: 0.5713323043655254
diningtable: 0.696122825413885
dog: 0.7490342862830122
horse: 0.8064323730818508
motorbike: 0.7576028544694091
person: 0.6886720836774618
pottedplant: 0.3843043827262724
sheep: 0.5665584224827491
sofa: 0.7792946515148939
train: 0.7889272422973491
tvmonitor: 0.5494204663075016

Average Precision Across All Classes:0.6517615617998829</pre>
    <p>&nbsp;</p>
  </section>

  <p>&nbsp;</p>

  <section>
    <h3><a name="4-2-2._MobileNetV3-large-SSD-Lite_を動かす（PC_内蔵カメラ）">4-2-2. 
    MobileNetV3-large-SSD-Lite を動かす（PC 内蔵カメラ）</a></h3>
    <h4>[概要]</h4>
    <p>MobileNetV3-large-SSD-lite を PyTorch の環境で動かしてみます。</p>
    <p>PC 内蔵カメラの映像を接続してリアルタイムで物体検知してみます。</p>
    <p>&nbsp;</p>
    <h4>[評価環境]</h4>
    <table>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>言語 :</td>
        <td>Python,</td>
        <td>3.10.7</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
        <td>PyTorch,</td>
        <td>1.13.1+cpu</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>OS :</td>
        <td>Windows 11 home,</td>
        <td>21H2</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
    </table>
    <p>&nbsp;</p>

    <h4>[手順]</h4>
    <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> から git clone してソースコード一式を入手します。</p>
    <p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
    <p>&nbsp;</p>
    <p>2. "git clone" したフォルダへ移動します。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ cd pytorch-ssd</pre>
    <p>&nbsp;</p>
    <p>3. 学習済みデータなどを取得します。</p>
    <p>(1) "<a href="mobilenet-ssd/mb3-large-ssd-lite-mp-0_652.pth">こちら</a>" 
    から学習済みデータ "<a href="mobilenet-ssd/mb3-large-ssd-lite-mp-0_652.pth">mb3-large-ssd-lite-mp-0_652.pth</a>" 
    を取得して models フォルダに保存します。</p>
    <p>(2) 下記 Google Drive から "voc-model-labels.txt" ファイルを取得して models 
    フォルダに保存します。</p>
    <p>
    <a href="https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu" target="_blank">
    https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu</a></p>
    <p>&nbsp;</p>
    <p>4. 下記コマンドを実行することで推論を実行します。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ python run_ssd_live_demo.py mb3-large-ssd-lite models/mb3-large-ssd-lite-mp-0_652.pth models/voc-model-labels.txt</pre>
    <p>&nbsp;</p>
    <p>学習済みモデルは VOC dataset を使って学習しているので、このプログラムは下記 20クラス を検知します。</p>
    <pre>1: aeroplane
2: bicycle
3: bird
4: boat
5: bottle
6: bus
7: car
8: cat
9: chair
10: cow
11: diningtable
12: dog
13: horse
14: motorbike
15: person
16: pottedplant
17: sheep
18: sofa
19: train
20: tvmonitor</pre>
    <p>&nbsp;</p>
    <p>5. プログラムを動かした様子を以下に示します。</p>
    <p>カメラはPC内蔵のカメラです。PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。</p>
    <p>&nbsp;</p>
    <p>[動画] プログラムを動作させた様子（PC 内蔵カメラ）</p>
    <video controls muted autoplay="y" loop="y" width="800">
      <source src="mobilenet-ssd/mobilenetv3-large-ssd-lite-pccam.mp4" type="video/mp4">
      動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。
    </video>
    <p>&nbsp;</p>
  </section>

  <p>&nbsp;</p>

  <section>
    <h3><a name="4-2-3._MobileNetV3-large-SSD-Lite_を動かす（i-PRO_カメラ）">4-2-3. 
    MobileNetV3-large-SSD-Lite を動かす（i-PRO カメラ）</a></h3>
    <h4>[概要]</h4>
    <p>MobileNetV3-large-SSD-lite を PyTorch の環境で動かしてみます。</p>
    <p>i-PRO カメラとPCを LAN 接続してリアルタイムで物体検知してみます。</p>
    <p>&nbsp;</p>
    <h4>[評価環境]</h4>
    <table>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>言語 :</td>
        <td>Python,</td>
        <td>3.10.7</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>&nbsp;</td>
        <td>PyTorch,</td>
        <td>1.13.1+cpu</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
      <tr>
        <td>OS :</td>
        <td>Windows 11 home,</td>
        <td>21H2</td>
      </tr>
      <tr>
        <td class="td_separate" colspan="3"></td>
      </tr>
    </table>
    <p>&nbsp;</p>
    
    <h4>[手順]</h4>
    <p>1. <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
    pytorch-ssd</a> から git clone してソースコード一式を入手します。</p>
    <p>任意のフォルダで端末（コマンドプロンプト等）を起動して下記コマンドをインプットします。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ git clone https://github.com/qfgaohao/pytorch-ssd.git</pre>
    <p>&nbsp;</p>
    <p>2. "git clone" したフォルダへ移動します。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ cd pytorch-ssd</pre>
    <p>&nbsp;</p>
    <p>3. 学習済みデータなどを取得します。</p>
    <p>(1) "<a href="mobilenet-ssd/mb3-large-ssd-lite-mp-0_652.pth">こちら</a>" 
    から学習済みデータ "<a href="mobilenet-ssd/mb3-large-ssd-lite-mp-0_652.pth">mb3-large-ssd-lite-mp-0_652.pth</a>" 
    を取得して models フォルダに保存します。</p>
    <p>(2) 下記 Google Drive から "voc-model-labels.txt" ファイルを取得して models 
    フォルダに保存します。</p>
    <p>
    <a href="https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu" target="_blank">
    https://drive.google.com/drive/folders/1pKn-RifvJGWiOx0ZCRLtCXM5GT5lAluu</a></p>
    <p>&nbsp;</p>
    <p>4. 下記コマンドを実行することで推論を実行します。ここでは４番目の引数として RTSP 表記で i-PRO カメラの接続を記載します。</p>
    <p><span class="auto-style3">{user-id}</span>, <span class="auto-style3">
    {password}</span>, <span class="auto-style3">{ip-address}</span> 
    の部分をご自身が使われる i-PRO カメラの設定に合わせて修正して実行してください。</p>
    <p>その他 RTSP に関しては記事「<a href="connect_with_rtsp.html">RTSP で画像を取得する</a>」を参照ください。</p>
    <pre style="color: #FFFFFF; background-color: #000000">$ python run_ssd_live_demo.py mb3-large-ssd-lite models/mb3-large-ssd-lite-mp-0_652.pth models/voc-model-labels.txt <span class="auto-style4">rtsp://</span><span class="auto-style5">{user-id}</span>:<span class="auto-style5">{password}</span>@<span class="auto-style5">{ip-address}</span><span class="auto-style4">/MediaInput/stream_1</span></pre>
    <p>(例) <span class="cpp-source">python run_ssd_live_demo.py 
    mb3-large-ssd-lite models/mb3-large-ssd-lite-mp-0_652.pth 
    models/voc-model-labels.txt 
    rtsp://admin:Admin12345@192.168.0.10/MediaInput/stream_1</span></p>
    <p>&nbsp;</p>
    <p>5. プログラムを動かした様子を以下に示します。</p>
    <p>PyTorch は前述の通り CPU 版ですがかなり快適に動作できています。i-PRO カメラの設定を 10fps 
    にしています。私のノートPC環境では 30fps 動作させると映像が少しずつ遅延していきました。AI処理なしに映像表示させると 30fps 
    表示できているので、AI処理に伴うCPU負荷に原因するものと分析します。GPU無しの環境、CPU 版での動作で Full-HD 画像を 10fps 
    でAI処理できているのですから、私個人の見解ですが、 MobileNet は十分に軽量で高性能な AI だと考えます。</p>
    <p>&nbsp;</p>
    <p>[動画] プログラムを動作させた様子（i-PRO カメラ）</p>
    <video controls muted autoplay="y" loop="y" width="800">
      <source src="mobilenet-ssd/mobilenetv3-large-ssd-lite-ipromini_1.mp4" type="video/mp4">
      動画を再生するには &lt;video&gt; タグをサポートしたブラウザが必要です。
    </video>  
    <p>&nbsp;</p>
    <p>&nbsp;</p>
    <p>以上です。というわけで、プログラムを１行も書くことなく i-PRO カメラとPCを接続して MobileNetV3-large-SSD-lite 
    も実行することができました。</p>
    <p>&nbsp;</p>
    <p>&nbsp;</p>
    <div class="status_information">
      <div>
      </div>
      <div>
        <p><strong>NOTE</strong></p>
        <p>この節では V1, V2 と同じ動作条件で比較することを目的に VOC dataset により学習を行った V3 
        学習データを作成し、動作させてみました。私の環境では V1、V2 と比較して V3 が最も動作が軽い印象です。認識するクラス数に比例して AI 
        処理負荷も重たくなる、ということだと推察します。ご自身で独自のAI学習データを作成する際は、必要なクラスのみでAI学習データを作成するようにすると良さそうです。</p>
      </div>
    </div>
  </section>
</section>
<p>&nbsp;</p>

<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
  <div class="status_information">
    <div>
    </div>
    <div>
      <p><strong>NOTE</strong></p>
      <p>続く５章は別ページ「<a href="mobilenet-ssd_train.html">物体検知 － MobileNet-SSD (学習編)</a>」で記載します。</p>
    </div>
  </div>
</section>

<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>

<section>
  <h2><a name="ソースコード所在">ソースコード所在</a></h2>
  <p>本ページで紹介のソースコードは、下記 github より取得できます。</p>
  <p>下記 github のソースコードと本ページの内容は差異がある場合があります。</p>
  <p><a href="https://github.com/i-pro-corp/python-examples" target="_blank">i-pro-corp/python-examples: Examples for i-PRO cameras. (github.com)</a></p>
</section>
<p>&nbsp;</p>

<section>
  <h2><a name="ライセンス">ライセンス</a></h2>
  <p>本ページの情報は、特記無い限り下記ライセンスで提供されます。</p>
  <div class="license">
    <br>Copyright 2022 i-PRO Co., Ltd.<br><br>Licensed under the Apache License, Version 
    2.0 (the "License");<br>you may not use this file except in compliance with 
    the License.<br>You may obtain a copy of the License at <br><br>&nbsp;&nbsp;&nbsp;
    <a href="http://www.apache.org/licenses/LICENSE-2.0" target="_blank">http://www.apache.org/licenses/LICENSE-2.0</a><br><br>
    Unless required by 
    applicable law or agreed to in writing, software <br>distributed under the 
    License is distributed on an "AS IS" BASIS, <br>WITHOUT WARRANTIES OR 
    CONDITIONS OF ANY KIND, either express or implied. <br>See the License for 
    the specific language governing permissions and<br>limitations under the 
    License. <br> <br>
  </div>
  <p>&nbsp;</p>
</section>

<p>&nbsp;</p>

<section>
	<h2><a name="参考">参考</a></h2>
	<ul>
		<li>[1] PyTorch<br><a href="https://pytorch.org/" target="_blank">
      https://pytorch.org/</a></li>
    <li>[2] qfgaohao/pytorch-ssd: MobileNetV1, MobileNetV2, VGG based 
      SSD/SSD-lite implementation in Pytorch 1.0 / Pytorch 0.4. Out-of-box 
      support for retraining on Open Images dataset. ONNX and Caffe2 support. 
      Experiment Ideas like CoordConv. (github.com)<br>
      <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">
      https://github.com/qfgaohao/pytorch-ssd</a></li>
    <li>[3] PyTorchでMobileNet SSDによるリアルタイム物体検出｜はやぶさの技術ノート (cpp-learning.com)<br>
      <a href="https://cpp-learning.com/pytorch_mobilenet-ssd/" target="_blank">
      https://cpp-learning.com/pytorch_mobilenet-ssd/</a></li>
    <li>[4] MobilenetSSD : 高速に物体検出を行う機械学習モデル. ailia… | by Kazuki Kyakuno | axinc | Medium<br>
      <a href="https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411" target="_blank">
      https://medium.com/axinc/mobilenetssd-高速に物体検出を行う機械学習モデル-be3ca37c411</a></li>
		<li>[5] Everything You Need To Know About Torchvision’s SSDlite Implementation | PyTorch<br>
      <a href="https://pytorch.org/blog/torchvision-ssdlite-implementation/" target="_blank">
      https://pytorch.org/blog/torchvision-ssdlite-implementation/</a></li>
		<li>[6] vision/ssdlite.py at b6f733046c9259f354d060cd808241a558d7d596 · pytorch/vision · GitHub<br>
  		<a href="https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L159-L162" target="_blank">
	  	https://github.com/pytorch/vision/blob/b6f733046c9259f354d060cd808241a558d7d596/torchvision/models/detection/ssdlite.py#L159-L162</a></li>
		<li>[7] Models and pre-trained weights — Torchvision main documentation (pytorch.org)<br>
		  <a href="https://pytorch.org/vision/main/models.html" target="_blank">
		  https://pytorch.org/vision/main/models.html</a></li>
		<li>[8] Visualization utilities — Torchvision 0.11.0 documentation (pytorch.org)<br>
      <a href="https://pytorch.org/vision/0.11/auto_examples/plot_visualization_utils.html" target="_blank">
      https://pytorch.org/vision/0.11/auto_examples/plot_visualization_utils.html</a></li>
		<li>[9] PyTorchでObeject Detection (mashykom.com)<br>
      <a href="https://www.koi.mashykom.com/pytorch2.html" target="_blank">
      https://www.koi.mashykom.com/pytorch2.html</a></li>
		<li>[10] SSDLite MobileNetV3 Backbone Object Detection with PyTorch and 
		Torchvision - DebuggerCafe<br>
		<a href="https://debuggercafe.com/ssdlite-mobilenetv3-backbone-object-detection-with-pytorch-and-torchvision/" target="_blank">
		https://debuggercafe.com/ssdlite-mobilenetv3-backbone-object-detection-with-pytorch-and-torchvision/</a></li>
		<li>[11] MobileNets: Efficient Convolutional Neural Networks for Mobile 
		Vision Applications <br>
		<a href="https://arxiv.org/pdf/1704.04861.pdf" target="_blank">
		https://arxiv.org/pdf/1704.04861.pdf</a></li>
		<li>[12] MobileNetV2: Inverted Residuals and Linear Bottlenecks <br>
		<a href="https://arxiv.org/pdf/1801.04381v3.pdf" target="_blank">
		https://arxiv.org/pdf/1801.04381v3.pdf</a></li>
		<li>[13] Searching for MobileNetV3 <br>
		<a href="https://arxiv.org/pdf/1905.02244.pdf" target="_blank">
		https://arxiv.org/pdf/1905.02244.pdf</a></li>
	</ul>
</section>

<p>&nbsp;</p>

<hr>

<p>&nbsp;</p>

<section>
	<h2 style="margin-bottom:5px">変更履歴</h2>
	<table>
	  <tr>
	    <td class="td_history_date">2023/5/12</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">「<a href="#4-2._MobileNetV3-large-SSD-Lite">4-2._MobileNetV3-large-SSD-Lite</a>」を追加,</td>
      <td class="td_history">木下英俊</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">2023/4/12</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">サイト <a href="https://github.com/qfgaohao/pytorch-ssd" target="_blank">pytorch-ssd</a> の更新に伴う記事更新,</td>
      <td class="td_history">木下英俊</td>
	  </tr>
	  <tr>
	    <td class="td_history_date">2023/3/1</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">説明および表現を一部更新,</td>
	    <td class="td_history">木下英俊</td>
	  </tr>
    <tr>
	    <td class="td_history_date">2022/10/5</td>
	    <td class="td_history_separator">-</td>
	    <td class="td_history">新規作成,</td>
	    <td class="td_history">木下英俊</td>
	  </tr>
	</table>
</section>

<p>&nbsp;</p>

<section>
<p><a href="../../index.html" target="_parent">Programming Items トップページ</a></p>
<p><a href="../../privacy_policy.html">プライバシーポリシー</a></p>
</section>

<p>&nbsp;</p>

<footer>
	<p><small>&copy; 2022  i-PRO Co., Ltd.</small></p>
</footer>

</body>
</html>
